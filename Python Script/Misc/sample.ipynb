{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int\n",
      "<class 'int'>\n",
      "string\n",
      "<class 'str'>\n",
      "int\n",
      "<class 'int'>\n",
      "['int', 'string', 'int']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, column\n",
    "spark = SparkSession.builder.master(\"local[1]\")\\\n",
    "          .appName(\"SparkByExample\")\\\n",
    "          .getOrCreate()\n",
    "df = spark.read.csv(\"Demo.csv\", header='True', inferSchema='True')\n",
    "column_type = []\n",
    "for col in df.dtypes:\n",
    "    column_type.append(col[1])\n",
    "    # print(col[0]+\" , \"+col[1])\n",
    "new_data = [1,'abc',7]\n",
    "for cl in range(len(column_type)):\n",
    "    print(column_type[cl])\n",
    "    print(type(new_data[cl]))\n",
    "    # if(type(df[cl]) != type(new_data[cl])):\n",
    "    #     raise Exception('Data type mismatch')\n",
    "print(column_type)\n",
    "newRow = spark.createDataFrame([new_data], column_header)\n",
    "new_df = df.union(newRow)\n",
    "new_df.show()\n",
    "\n",
    "# columns = df.header\n",
    "# newRow = spark.createDataFrame([(3,205,7)], columns)\n",
    "# print(df)\n",
    "# df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame schema:  StructType([StructField('Id', StringType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True)])\n",
      "Expected Schema:  StructType([StructField('Id', IntegerType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True)])\n",
      "Schema validation failed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), nullable=True),\n",
    "    StructField(\"Name\", StringType(), nullable=True),\n",
    "    StructField(\"Name_id\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"Demo.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print('Data frame schema: ',df.schema)\n",
    "print('Expected Schema: ',expected_schema)\n",
    "\n",
    "if df.schema == expected_schema:\n",
    "    print(\"Schema validation successful\")\n",
    "else:\n",
    "    print(\"Schema validation failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame schema:  StructType([StructField('Id', StringType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True)])\n",
      "Expected Schema:  StructType([StructField('Id', IntegerType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True)])\n",
      "Schema validation failed.\n",
      "Mismatched columns:  set()\n",
      "Mismatched rows:  10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), nullable=True),\n",
    "    StructField(\"Name\", StringType(), nullable=True),\n",
    "    StructField(\"Name_id\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"Demo.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print('Data frame schema: ', df.schema)\n",
    "print('Expected Schema: ', expected_schema)\n",
    "\n",
    "if df.schema == expected_schema:\n",
    "    print(\"Schema validation successful\")\n",
    "else:\n",
    "    mismatched_columns = set(df.schema.fieldNames()).symmetric_difference(set(expected_schema.fieldNames()))\n",
    "    mismatched_rows = df.exceptAll(spark.createDataFrame([], schema=df.schema)).count()\n",
    "    print(\"Schema validation failed.\")\n",
    "    print(\"Mismatched columns: \", mismatched_columns)\n",
    "    print(\"Mismatched rows: \", mismatched_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame schema:  StructType([StructField('Id', StringType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True)])\n",
      "Expected Schema:  StructType([StructField('Id', IntegerType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True)])\n",
      "Schema validation failed.\n",
      "Mismatched columns:  set()\n",
      "Mismatched rows:\n",
      "Row ID:  7\n",
      "Row ID:  4\n",
      "Row ID:  2\n",
      "Row ID:  9\n",
      "Row ID:  6\n",
      "Row ID:  5\n",
      "Row ID:  8\n",
      "Row ID:  0\n",
      "Row ID:  3\n",
      "Row ID:  1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), nullable=True),\n",
    "    StructField(\"Name\", StringType(), nullable=True),\n",
    "    StructField(\"Name_id\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"Demo.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print('Data frame schema: ', df.schema)\n",
    "print('Expected Schema: ', expected_schema)\n",
    "\n",
    "if df.schema == expected_schema:\n",
    "    print(\"Schema validation successful\")\n",
    "else:\n",
    "    mismatched_columns = set(df.schema.fieldNames()).symmetric_difference(set(expected_schema.fieldNames()))\n",
    "\n",
    "    df = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "    expected_df = spark.createDataFrame([], schema=df.schema).withColumn(\"row_id\", monotonically_increasing_id())\n",
    "    mismatched_rows = df.exceptAll(expected_df).select(\"row_id\").collect()\n",
    "\n",
    "    print(\"Schema validation failed.\")\n",
    "    print(\"Mismatched columns: \", mismatched_columns)\n",
    "    print(\"Mismatched rows:\")\n",
    "    for row in mismatched_rows:\n",
    "        print(\"Row ID: \", row[\"row_id\"])\n",
    "\n",
    "# To get the specific row number, you can use the row_id as follows:\n",
    "# mismatched_row_number = row[\"row_id\"] + 1\n",
    "# print(\"Row Number: \", mismatched_row_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame schema:  StructType([StructField('Id', StringType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True)])\n",
      "Expected Schema:  StructType([StructField('Id', IntegerType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True)])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "property 'schema' of 'DataFrame' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSchema validation successful\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     df\u001b[39m.\u001b[39;49mschema \u001b[39m=\u001b[39m StructType([\n\u001b[1;32m     23\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39mId\u001b[39m\u001b[39m\"\u001b[39m, IntegerType(), nullable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     24\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39mName\u001b[39m\u001b[39m\"\u001b[39m, StringType(), nullable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     25\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39mName_id\u001b[39m\u001b[39m\"\u001b[39m, IntegerType(), nullable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m ])\n\u001b[1;32m     27\u001b[0m     expected_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame([], schema\u001b[39m=\u001b[39mexpected_schema)\n\u001b[1;32m     29\u001b[0m     mismatched_rows \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mexceptAll(expected_df)\u001b[39m.\u001b[39mselect(monotonically_increasing_id()\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mrow_number\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: property 'schema' of 'DataFrame' object has no setter"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), nullable=True),\n",
    "    StructField(\"Name\", StringType(), nullable=True),\n",
    "    StructField(\"Name_id\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"Demo.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print('Data frame schema: ', df.schema)\n",
    "print('Expected Schema: ', expected_schema)\n",
    "\n",
    "if df.schema == expected_schema:\n",
    "    print(\"Schema validation successful\")\n",
    "else:\n",
    "    df.schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), nullable=True),\n",
    "    StructField(\"Name\", StringType(), nullable=True),\n",
    "    StructField(\"Name_id\", IntegerType(), nullable=True)\n",
    "])\n",
    "    expected_df = spark.createDataFrame([], schema=expected_schema)\n",
    "\n",
    "    mismatched_rows = df.exceptAll(expected_df).select(monotonically_increasing_id().alias(\"row_number\"))\n",
    "\n",
    "    print(\"Schema validation failed.\")\n",
    "    print(\"Mismatched rows:\")\n",
    "    mismatched_rows.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame schema:  StructType([StructField('Id', StringType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True), StructField(' asd', IntegerType(), True)])\n",
      "Expected Schema:  StructType([StructField('Id', IntegerType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True)])\n",
      "Schema validation failed.\n",
      "Mismatched columns:  {' asd'}\n",
      "Sample data from mismatched columns:\n",
      "Column:  asd\n",
      "+----+\n",
      "| asd|\n",
      "+----+\n",
      "|12  |\n",
      "|23  |\n",
      "|null|\n",
      "|null|\n",
      "|45  |\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), nullable=True),\n",
    "    StructField(\"Name\", StringType(), nullable=True),\n",
    "    StructField(\"Name_id\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"Demo.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print('Data frame schema: ', df.schema)\n",
    "print('Expected Schema: ', expected_schema)\n",
    "\n",
    "if df.schema == expected_schema:\n",
    "    print(\"Schema validation successful\")\n",
    "else:\n",
    "    mismatched_columns = set(df.schema.fieldNames()).symmetric_difference(set(expected_schema.fieldNames()))\n",
    "    print(\"Schema validation failed.\")\n",
    "    print(\"Mismatched columns: \", mismatched_columns)\n",
    "    print(\"Sample data from mismatched columns:\")\n",
    "\n",
    "    # Show sample data from mismatched columns\n",
    "    for column_name in mismatched_columns:\n",
    "        print(f\"Column: {column_name}\")\n",
    "        df.select(column_name).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame schema:  StructType([StructField('Id', StringType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True), StructField('asdas', IntegerType(), True)])\n",
      "Expected Schema:  StructType([StructField('Id', IntegerType(), True), StructField('Name', StringType(), True), StructField('Name_id', IntegerType(), True)])\n",
      "Schema validation failed.\n",
      "Mismatched row:\n",
      "+----+-----+-------+-----+\n",
      "|  Id| Name|Name_id|asdas|\n",
      "+----+-----+-------+-----+\n",
      "|asdf|Sudip|      5| null|\n",
      "+----+-----+-------+-----+\n",
      "\n",
      "Schema validation failed.\n",
      "Mismatched columns:  {'asdas'}\n",
      "Sample data from mismatched columns:\n",
      "Column: asdas\n",
      "+-----+\n",
      "|asdas|\n",
      "+-----+\n",
      "|13   |\n",
      "|123  |\n",
      "|43   |\n",
      "|null |\n",
      "|null |\n",
      "|45   |\n",
      "|67   |\n",
      "|null |\n",
      "|null |\n",
      "|null |\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), nullable=True),\n",
    "    StructField(\"Name\", StringType(), nullable=True),\n",
    "    StructField(\"Name_id\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"Demo.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print('Data frame schema: ', df.schema)\n",
    "print('Expected Schema: ', expected_schema)\n",
    "\n",
    "if df.schema == expected_schema:\n",
    "    print(\"Schema validation successful\")\n",
    "else:\n",
    "    mismatched_row = df.filter(~(df[\"Id\"].cast(IntegerType()).isNotNull() &\n",
    "                                 df[\"Name\"].cast(StringType()).isNotNull() &\n",
    "                                 df[\"Name_id\"].cast(IntegerType()).isNotNull()))\n",
    "    if  mismatched_row.count() > 0:\n",
    "        print(\"Schema validation failed.\")\n",
    "        print(\"Mismatched row:\")\n",
    "        mismatched_row.show()\n",
    "        mismatched_columns = set(df.schema.fieldNames()).symmetric_difference(set(expected_schema.fieldNames()))\n",
    "    if  len(mismatched_columns)>0:\n",
    "        print(\"Schema validation failed.\")\n",
    "        print(\"Mismatched columns: \", mismatched_columns)\n",
    "        print(\"Sample data from mismatched columns:\")\n",
    "        # Show sample data from mismatched columns\n",
    "        for column_name in mismatched_columns:\n",
    "            print(f\"Column: {column_name}\")\n",
    "            df.select(column_name).show(10, truncate=False)\n",
    "\n",
    "# if all(col in df.columns for col in expected_schema.fieldNames()):\n",
    "#     mismatched_row = df.filter(~(df[\"Id\"].cast(IntegerType()).isNotNull() &\n",
    "#                                  df[\"Name\"].cast(StringType()).isNotNull() &\n",
    "#                                  df[\"Name_id\"].cast(IntegerType()).isNotNull()))\n",
    "\n",
    "#     if mismatched_row.count() > 0:\n",
    "#         print(\"Schema validation failed.\")\n",
    "#         print(\"Mismatched row:\")\n",
    "#         mismatched_row.show()\n",
    "#     else:\n",
    "#         print(\"Schema validation successful\")\n",
    "# else:\n",
    "#     print(\"Schema validation failed. Mismatched columns.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
