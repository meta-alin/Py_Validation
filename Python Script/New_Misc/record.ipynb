{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alin/.local/lib/python3.11/site-packages/pyspark/sql/pandas/group_ops.py:103: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "23/05/26 16:26:09 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 5)/ 1]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "                          ^^^^^^^^^^^^^\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 202, in wrapped\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Number of columns of the returned pandas.DataFrame doesn't match specified schema. Expected: 3 Actual: 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/05/26 16:26:09 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 5) (192.168.4.185 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n",
      "    return f(keys, vals)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n",
      "    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n",
      "                          ^^^^^^^^^^^^^\n",
      "  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 202, in wrapped\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Number of columns of the returned pandas.DataFrame doesn't match specified schema. Expected: 3 Actual: 1\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/05/26 16:26:09 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n    for series in iterator:\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n    return f(keys, vals)\n           ^^^^^^^^^^^^^\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n                          ^^^^^^^^^^^^^\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 202, in wrapped\n    raise RuntimeError(\nRuntimeError: Number of columns of the returned pandas.DataFrame doesn't match specified schema. Expected: 3 Actual: 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m# Apply the PySpark Pandas UDF and count the number of rows\u001b[39;00m\n\u001b[1;32m     26\u001b[0m processed_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mapply(my_pandas_udf)\n\u001b[0;32m---> 27\u001b[0m pandas_count_pyspark \u001b[39m=\u001b[39m processed_df\u001b[39m.\u001b[39;49mcount()\n\u001b[1;32m     29\u001b[0m \u001b[39m# Calculate the execution time for PySpark Pandas API\u001b[39;00m\n\u001b[1;32m     30\u001b[0m execution_time_pyspark \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time_pyspark\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcount())\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 345, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 86, in dump_stream\n    for batch in iterator:\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 338, in init_stream_yield_batches\n    for series in iterator:\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 593, in mapper\n    return f(keys, vals)\n           ^^^^^^^^^^^^^\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 209, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n                          ^^^^^^^^^^^^^\n  File \"/home/alin/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 202, in wrapped\n    raise RuntimeError(\nRuntimeError: Number of columns of the returned pandas.DataFrame doesn't match specified schema. Expected: 3 Actual: 1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"TimingComparison\").getOrCreate()\n",
    "\n",
    "# Read data from a CSV file\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Register the PySpark DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Define a PySpark Pandas UDF\n",
    "@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)\n",
    "def my_pandas_udf(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Perform data processing using Pandas DataFrame operations\n",
    "    processed_data = data[data['column'] > 10][['column1', 'column2']].groupby('column1').sum()\n",
    "    return processed_data\n",
    "\n",
    "# Start the timer for PySpark Pandas API\n",
    "start_time_pyspark = time.time()\n",
    "\n",
    "# Apply the PySpark Pandas UDF and count the number of rows\n",
    "processed_df = df.groupby('column').apply(my_pandas_udf)\n",
    "pandas_count_pyspark = processed_df.count()\n",
    "\n",
    "# Calculate the execution time for PySpark Pandas API\n",
    "execution_time_pyspark = time.time() - start_time_pyspark\n",
    "\n",
    "# Convert the processed DataFrame to Pandas DataFrame\n",
    "pandas_df = processed_df.toPandas()\n",
    "\n",
    "# Start the timer for pure Pandas\n",
    "start_time_pandas = time.time()\n",
    "\n",
    "# Perform additional data processing using Pandas DataFrame operations and count the number of rows\n",
    "pandas_result = pandas_df.groupby('column1').sum()\n",
    "pandas_count_pandas = pandas_result.shape[0]\n",
    "\n",
    "# Calculate the execution time for pure Pandas\n",
    "execution_time_pandas = time.time() - start_time_pandas\n",
    "\n",
    "# Display the counts and execution times\n",
    "print(\"PySpark Pandas API Count:\", pandas_count_pyspark)\n",
    "print(\"PySpark Pandas API Execution Time:\", execution_time_pyspark)\n",
    "\n",
    "print(\"Pure Pandas Count:\", pandas_count_pandas)\n",
    "print(\"Pure Pandas Execution Time:\", execution_time_pandas)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/26 16:55:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "ArithmeticException",
     "evalue": "/ by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArithmeticException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mmaster(\u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mTimingComparison\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.cores.max\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetOrCreate()\n\u001b[1;32m     10\u001b[0m \u001b[39m# Read data from a CSV file\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(\u001b[39m\"\u001b[39;49m\u001b[39mdata.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, inferSchema\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Register the PySpark DataFrame as a temporary table\u001b[39;00m\n\u001b[1;32m     14\u001b[0m df\u001b[39m.\u001b[39mcreateOrReplaceTempView(\u001b[39m\"\u001b[39m\u001b[39mmy_table\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:727\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    726\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    728\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    730\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mArithmeticException\u001b[0m: / by zero"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "import pandas as pd\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"TimingComparison\").config(\"spark.cores.max\", \"1\").getOrCreate()\n",
    "\n",
    "# Read data from a CSV file\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Register the PySpark DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Define the schema for the output DataFrame\n",
    "output_schema = StructType([\n",
    "    StructField(\"column1\", StringType(), nullable=False),\n",
    "    StructField(\"column2\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Define a PySpark Pandas UDF\n",
    "@pandas_udf(output_schema, PandasUDFType.GROUPED_MAP)\n",
    "def my_pandas_udf(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Perform data processing using Pandas DataFrame operations\n",
    "    processed_data = data.groupby('column1')['column2'].sum().reset_index()\n",
    "    return processed_data\n",
    "\n",
    "# Start the timer for PySpark Pandas API\n",
    "start_time_pyspark = time.time()\n",
    "\n",
    "# Apply the PySpark Pandas UDF and count the number of rows\n",
    "processed_df = df.groupby('column1').apply(my_pandas_udf)\n",
    "pandas_count_pyspark = processed_df.count()\n",
    "\n",
    "# Calculate the execution time for PySpark Pandas API\n",
    "execution_time_pyspark = time.time() - start_time_pyspark\n",
    "\n",
    "# Convert the processed DataFrame to Pandas DataFrame\n",
    "pandas_df = processed_df.toPandas()\n",
    "\n",
    "# Start the timer for pure Pandas\n",
    "start_time_pandas = time.time()\n",
    "\n",
    "# Perform additional data processing using Pandas DataFrame operations and count the number of rows\n",
    "pandas_result = pandas_df.groupby('column1')['column2'].sum().reset_index()\n",
    "pandas_count_pandas = pandas_result.shape[0]\n",
    "\n",
    "# Calculate the execution time for pure Pandas\n",
    "execution_time_pandas = time.time() - start_time_pandas\n",
    "\n",
    "# Display the counts and execution times\n",
    "print(\"PySpark Pandas API Count:\", pandas_count_pyspark)\n",
    "print(\"PySpark Pandas API Execution Time:\", execution_time_pyspark)\n",
    "\n",
    "print(\"Pure Pandas Count:\", pandas_count_pandas)\n",
    "print(\"Pure Pandas Execution Time:\", execution_time_pandas)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/26 17:06:15 WARN SparkConf: Total executor cores: 1 is not divisible by cores per executor: 4, the left cores: 1 will not be allocated\n",
      "/home/alin/.local/lib/python3.11/site-packages/pyspark/sql/pandas/group_ops.py:103: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Pandas API Count: 4\n",
      "PySpark Pandas API Execution Time: 13.98938250541687\n",
      "Pure Pandas Count: 4\n",
      "Pure Pandas Execution Time: 0.0017251968383789062\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "import pandas as pd\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"TimingComparison\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.executor.memory\", \"1g\")\n",
    "spark.conf.set(\"spark.executor.cores\", \"1\")\n",
    "\n",
    "# Read data from a CSV file\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Register the PySpark DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Define the schema for the output DataFrame\n",
    "output_schema = StructType([\n",
    "    StructField(\"column1\", StringType(), nullable=False),\n",
    "    StructField(\"column2\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Define a PySpark Pandas UDF\n",
    "@pandas_udf(output_schema, PandasUDFType.GROUPED_MAP)\n",
    "def my_pandas_udf(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Perform data processing using Pandas DataFrame operations\n",
    "    processed_data = data.groupby('column1')['column2'].sum().reset_index()\n",
    "    return processed_data\n",
    "\n",
    "# Start the timer for PySpark Pandas API\n",
    "start_time_pyspark = time.time()\n",
    "\n",
    "# Apply the PySpark Pandas UDF and count the number of rows\n",
    "processed_df = df.groupby('column1').apply(my_pandas_udf)\n",
    "pandas_count_pyspark = processed_df.count()\n",
    "\n",
    "# Calculate the execution time for PySpark Pandas API\n",
    "execution_time_pyspark = time.time() - start_time_pyspark\n",
    "\n",
    "# Convert the processed DataFrame to Pandas DataFrame\n",
    "pandas_df = processed_df.toPandas()\n",
    "\n",
    "# Start the timer for pure Pandas\n",
    "start_time_pandas = time.time()\n",
    "\n",
    "# Perform additional data processing using Pandas DataFrame operations and count the number of rows\n",
    "pandas_result = pandas_df.groupby('column1')['column2'].sum().reset_index()\n",
    "pandas_count_pandas = pandas_result.shape[0]\n",
    "\n",
    "# Calculate the execution time for pure Pandas\n",
    "execution_time_pandas = time.time() - start_time_pandas\n",
    "\n",
    "# Display the counts and execution times\n",
    "print(\"PySpark Pandas API Count:\", pandas_count_pyspark)\n",
    "print(\"PySpark Pandas API Execution Time:\", execution_time_pyspark)\n",
    "\n",
    "print(\"Pure Pandas Count:\", pandas_count_pandas)\n",
    "print(\"Pure Pandas Execution Time:\", execution_time_pandas)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
